{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a808d0e",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Illusory Pitch Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbd600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from glob import glob\n",
    "\n",
    "def dprime_and_c(hit_rate, fa_rate):\n",
    "    \n",
    "    # Get corresponding z-scores for the hit rate and false alarm rate\n",
    "    zH = stats.norm.ppf(hit_rate)\n",
    "    zF = stats.norm.ppf(fa_rate)\n",
    "    \n",
    "    # Calculate d' and C using z-scores\n",
    "    dprime = zH - zF\n",
    "    C = -(zH + zF) / 2\n",
    "    \n",
    "    return dprime, C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128fd80",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c327fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all data files\n",
    "datafiles = glob('../data/IPAD_*.csv')\n",
    "\n",
    "# Load each data file and concatenate them into a single table\n",
    "d = pd.concat((pd.read_csv(f) for f in datafiles))\n",
    "\n",
    "# Select only non-pilot participants\n",
    "#d = d[d.code_version == 1]\n",
    "\n",
    "# Select only main trial events\n",
    "d = d[d.event == 'trial']\n",
    "\n",
    "# Recode the pitch shift and key press as 0 for \"lower\" and 1 for \"higher\"\n",
    "d = d.assign(answer = d['shift'] == '+',\n",
    "            response = d['response'] == '+')\n",
    "\n",
    "# Before version 1.1, the experiment always loaded the stimuli for difficulty 1.0, so manually overwrite recorded trial difficulty\n",
    "d.loc[d.version < 1.1, 'difficulty'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53546f1",
   "metadata": {},
   "source": [
    "### Calculate scores within each subject and condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conditions as (octave, offset) pairs\n",
    "\n",
    "\n",
    "# Scores will be stored in a long-format table\n",
    "scores = pd.DataFrame(columns=['subject', 'experimenter', 'version', 'jnd', 'difficulty', 'interval',\n",
    "                               'hit_rate', 'fa_rate', 'accuracy', 'perc_resp_low', 'dprime', 'C', 'rt'])\n",
    "\n",
    "# Calculate scores for each subject\n",
    "for s, subj in enumerate(d.subject.unique()):\n",
    "\n",
    "    # Select all responses from the current subject\n",
    "    subj_trials = d[d.subject == subj]\n",
    "    experimenter = subj_trials.iloc[-1].experimenter\n",
    "    exp_version = subj_trials.iloc[-1].version\n",
    "    jnd = subj_trials.iloc[-1].jnd\n",
    "    conditions = [(1, 425), (1, 500), (1, 575), (0.5, 425), (0.5, 500), (0.5, 575)]\n",
    "\n",
    "    # Calculate scores within each condition\n",
    "    for i, condition in enumerate(conditions):\n",
    "        \n",
    "        # Select all trials from the current condition\n",
    "        difficulty = condition[0]\n",
    "        interval = condition[1]\n",
    "        trials = subj_trials[(subj_trials.difficulty == difficulty) & (subj_trials.interval == interval)]\n",
    "        if len(trials) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create dictionary to store scores from current subject and condition\n",
    "        condi_scores = dict(subject=subj, experimenter=experimenter, version=exp_version, difficulty=difficulty, interval=interval)\n",
    "        \n",
    "        # Calculate accuracy and the percent of the time the participant responded \"lower\"\n",
    "        condi_scores['accuracy'] = np.mean(trials.correct)\n",
    "        condi_scores['perc_resp_low'] = np.mean(~trials.response)\n",
    "        \n",
    "        # Calculate hit and false alarm rates using Hautus (1995) adjustment to avoid 0s and 1s\n",
    "        condi_scores['hit_rate'] = (np.sum(trials.answer & trials.response) + .5) / (np.sum(trials.answer) + 1)\n",
    "        condi_scores['fa_rate'] = (np.sum(~trials.answer & trials.response) + .5) / (np.sum(~trials.answer) + 1)\n",
    "        \n",
    "        # Calculate d' and C based on the hit rate and false alarm rate\n",
    "        condi_scores['dprime'], condi_scores['C'] = dprime_and_c(condi_scores['hit_rate'], condi_scores['fa_rate'])\n",
    "\n",
    "        condi_scores['rt'] = trials.rt.mean()\n",
    "        condi_scores['jnd'] = jnd\n",
    "        \n",
    "        # Add current scores as a row to the full table of scores\n",
    "        scores.loc[len(scores.index)] = condi_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1ff59",
   "metadata": {},
   "source": [
    "### Calculate extra scores for exploratory analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8904771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define offset conditions\n",
    "intervals = [425, 500, 575]\n",
    "\n",
    "# Scores will be stored in a data frame with one row per subject\n",
    "exploratory_scores = pd.DataFrame(columns=['subject', 'jnd', 'dprime', 'csize'])\n",
    "\n",
    "uniq_subjs = d.subject.unique()\n",
    "subj_dprimes = np.empty(len(uniq_subjs))\n",
    "subj_Cs = np.empty(len(uniq_subjs))\n",
    "subj_effectsizes = np.empty(len(uniq_subjs))\n",
    "for s, subj in enumerate(uniq_subjs):\n",
    "    \n",
    "    # Select all responses from the current subject\n",
    "    subj_trials = d[d.subject == subj]\n",
    "    jnd = subj_trials.iloc[-1].jnd\n",
    "    \n",
    "    # Calculate person's overall sensitivity\n",
    "    subj_hit_rate = (np.sum(subj_trials.answer & subj_trials.response) + .5) / (np.sum(subj_trials.answer) + 1)\n",
    "    subj_fa_rate = (np.sum(~subj_trials.answer & subj_trials.response) + .5) / (np.sum(~subj_trials.answer) + 1)\n",
    "    dprime, _ = dprime_and_c(subj_hit_rate, subj_fa_rate)\n",
    "    \n",
    "    # Calculate overall timing-induced bias size\n",
    "    C = np.full(len(intervals), np.nan)\n",
    "    for i, interval in enumerate(intervals):\n",
    "        trials = subj_trials[subj_trials.interval == interval]\n",
    "        offset_hit_rate = (np.sum(trials.answer & trials.response) + .5) / (np.sum(trials.answer) + 1)\n",
    "        offset_fa_rate = (np.sum(~trials.answer & trials.response) + .5) / (np.sum(~trials.answer) + 1)\n",
    "        _, C[i] = dprime_and_c(offset_hit_rate, offset_fa_rate)\n",
    "        csize = np.std(C) #np.mean(np.abs([C[0] - C[1], C[0] - C[2], C[1] - C[2]]))\n",
    "        \n",
    "    # Add new row of scores to the data frame\n",
    "    exploratory_scores.loc[len(exploratory_scores.index)] = dict(subject=subj, jnd=jnd, dprime=dprime, csize=csize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c90aa2",
   "metadata": {},
   "source": [
    "### Save processed scores to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.to_csv('../data/scores.csv', index=False)\n",
    "exploratory_scores.to_csv('../data/exploratory_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
